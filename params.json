{"name":"Hadoop Cube In Reducer Grouping","tagline":"","body":"CUBE\r\n====\r\nIn this project, I proposed a new algorithm for CUBE computation in distributed systems. Instead of dividing CUBE lattice into friendly and unfriendly patches like MRCUBE, the algorithm represents CUBE lattice by ROLLUPs. Then each ROLLUP is computed using IRG algorithm. This representation helps the algorithm overcome limits of MRCUBE which are not taking the advantage of sorting phase of most distributed systems (e.g. Hadoop, Spark...) and using a lot of memory in reducer phase where BUC is used. I used Map Reduce paradigm to implement and to demonstrate efficiency of the algorithm. Experiments are conducted with both synthetic and real data. Experiments show that the algorithm yields better performance and memory usage when compared with MRCUBE.\r\n\r\nRelated works\r\n-----------------\r\n- BUC:  **Bottom-Up Computation of Sparse and Iceberg CUBEs** of *Kevin Beyer and Raghu Ramakrishnan*. [For more information](https://github.com/nncsang/Hadoop-Cube-Bottom-Up-Computation)\r\n- MRCUBE: **Distributed Cube Materialization on Holistic Measures** of Arnab Nandi, Cong Yu, Philip Bohannon, Raghu Ramakrishnan. [For more information](https://github.com/nncsang/Hadoop-Cube-MRCube) \r\n- IRG: **On the design space of MapReduce ROLLUP aggregates** of *DH Phan, M Dell'Amico, P Michiardi*\r\n\r\nMRCube\r\n======\r\nIn this project, I implemented MRCube algorithm which is described in **\"Distributed Cube Materialization on Holistic Measures\"** of *Arnab Nandi, Cong Yu, Philip Bohannon, Raghu Ramakrishnan*.\r\n\r\nDataset\r\n======\r\n - http://stat-computing.org/dataexpo/2009/the-data.html: data from 1987\r\n - Uncompressed size:  127.2MB\r\n - Number of records: 1 311 827\r\n - Fields interested: Month, FlightNum (Flight number), Des (Destination), Distance (4 fields in total)\r\n\r\nSummary\r\n======\r\n - Cube computation over massive datasets is critical for many important analyses done in the real world\r\n - **Algebraic measures** (e.g. *SUM*) are easy to parallel. On the other hand, **holistic measures** (e.g. *REACH, TOP-K*) is non-trivial.\r\n - In the paper, the authors identified an important subset of holistic measures and introduced MR-Cube algorithm for efficient cube computation on these measures.\r\n\r\nData cube analysis\r\n======\r\nConsider a warehouse: **(city, state, country, day, month, year, sales)** in which:\r\n- *(city, state, country)*: location dimension\r\n- *(day, month, year)*: temporal dimension\r\nCube analysis computes aggregate measures (e.g. *sales*) over all possible groups defined by the two dimensions. \r\n\r\nThere are two main limitations in the existing techniques:\r\n- They designed for a **single machine** or **clusters with small number of nodes**. With the growing of data (terabytes accumulated per day), it is **difficult** to **process** data with that infrastructure.\r\n- Many of them **takes advantage** of the measure being **algebraic**.\r\n\r\nHow to **efficiently extend** cube analysis for **holistic measures** in **Map Reduce paradigm**? Existing problems:\r\n- *Effective distribute data*: avoid overwhelmed for any single machine --> addressed by **identifying the partially algebraic measures** and **value partition mechanism**.\r\n- *Effective distributing computation*: good balance between the amount of intermediate data being produced and the pruning unnecessary data --> addressed by **batch areas** \r\n\r\nDefinitions\r\n======\r\n- **Dimension attributes**: attributes that users want to analyze\r\n- **Cube lattice**: all possible grouping(s) of the attributes\r\n- **Cube region**: each node in cube lattice represents one possible grouping \r\n- **Cube group**: an actual tuple belonging to a cube region.\r\n- Each edge in the lattice represents a parent/child relationship between two cube regions or two cube groups\r\n- **Cubing task**: is to compute given measures for all valid cube groups\r\n- **Algebraic & Holistic & monotonic**: please find in the paper for the formal definitions\r\n\r\nChallenges\r\n======\r\n*Cube expressed in Pig* by disjunction of group-by queries, then it combines all queries into a single MapReduce job. This approach is simple but only efficient for small datasets. When the scale of data increases, this algorithm to perform poorly and eventually fail due to the huge size of intermediate data and size of large groups.\r\n\r\n- Size of Intermediate Data: |C| * |D|, where |C| is the number of regions in the cube lattice and |D| is the size of the input data\r\n- Size of Large Groups: The reducer that is assigned the cube regions at the bottom part of the cube lattice essentially has to compute the measure for the entire dataset, which is usually large enough to cause the reducer to take significantly longer time to finish than others or even fail. For algebraic measures, this challenge can addressed by not processing those groups directly: we can first compute measures only for those smaller, reducer-friendly, groups, then combine those measures to produce the measure for the larger, reducer-unfriendly, groups. Such measures are also amenable to mapper-side aggregation which further decreases the load on the shuffle and reduce phases. For holistic measures, however, measures for larger groups cannot be assembled from their smaller child groups, and mapper-side aggregation is also not possible. Hence, we need a different approach.\r\n\r\nThe MR-Cube approach\r\n======\r\n**Note**: the complexity of cubing tasks depends on:\r\n- **data size**: impacts intermediate data size, the size of large group\r\n- **cube lattice size** (is controlled by the number/depth of dimensions impacts intermediate data size\r\n\r\nMR-Cube approach deal with those complexities in a two-pronged attack: **data partitioning** and **cube lattice partitioning**\r\n\r\nPartially Algebraic Measures\r\n======\r\n- Purpose: to identify a subset of holistic measures that are easy to compute in **parallel** than an arbitrary holistic measure.\r\n- We call this technique of partitioning large groups based on the algebraic attribute **value partitioning**, and the ratio by which a group is partitioned the **partition factor**\r\n\r\nValue Partitioning\r\n======\r\nAn easy way to accomplish value partitioning is to run the naive algorithm, but further **partition each cube group based on the algebraic attribute**. The number of map keys being produced is now **the product of the number of groups and the partition factor**.\r\n\r\nObservations:\r\n- Many of the original groups contain a manageable number of tuples and partitioning those groups is entirely unnecessary\r\n- Even for large, reducer- unfriendly, groups, some will require partitioning into many sub-groups (i.e., large partition factor), while others will only need to be partitioned into a few sub-groups\r\n\r\nThe idea is to perform value partitioning only on groups that are likely to be **reducer-unfriendly** and **dynamically adjust the partition factor**\r\n\r\nApproaches:\r\n- Detect reducer unfriendly groups on the fly and perform partitioning upon detection -> overwhelm the mapper since it requires us to maintain information about groups visited.\r\n- Scan the data and compile a list of potentially reducer-unfriendly groups for which the mapper will perform partitioning -> Checking against a potentially large list slows down the mapper.\r\n\r\nBased on these analyses, the authors proposed **sampling approach**\r\n\r\nSampling Approach\r\n======\r\n- Estimate the **reducer-unfriendliness** of each **cube region** based on **the number of groups it is estimated to have**\r\n- Perform **partitioning for all groups** within the list of cube regions (a small list) that are estimated to be reducer **unfriendly**\r\n\r\nThis sampling is accomplished by performing cube computation using the naive algorithm on a small random subset of data, with count as the measure.\r\n\r\nWe declare a group G to be reducer-unfriendly if we observe more than 0.75rN tuples of G in the sample, where N is the sample size and r= c/ |D| denotes the maximum number of tuples a single reducer can handle (c) as a percentage of the overall data size (|D|). (See Proposition 1 in the paper for more details)\r\n\r\nA region is a reducer-unfriendly if it contains at least one reducer-unfriendly group. In addition, let the sample count of the largest reducer-unfriendly group in the region be s, we annotate the region with **the appropriate partition factor**, an integer that is closest to (s/(r * n))^3\r\n\r\nBatch Areas\r\n======\r\nGiven the annotated cube lattice, we can again directly apply the naive algorithm, process each cube group independently with the added safeguard that partitions the groups that belong to a reducer-unfriendly region. But **each tuple** is still duplicated **at least |C| times** and the naive approach is its **incompatibility** with **pruning for monotonic measures**. We need combine regions into batch areas.\r\n\r\nEach batch area represents **a collection of regions** that share a **common ancestor region**. Mappers can now emit **one key-value pair** per batch for each data tuple. Reducers, on the other hand, instead of simply applying the measure function, **execute a traditional cube computation algorithm** over the set of tuples using the batch area as the local cube lattice.\r\n\r\nReading more in the paper how to form batches\r\n\r\nMy experiment\r\n======\r\nNaive algorithm: \r\n- Number of intermediate keys: 20 989 216 = 1 311 826 * 2^4 \r\n- Size of intermediate keys: 2.54 GB\r\n- Time execution: 3 minutes\r\n\r\nMRCube algorithm:\r\n\r\nAssumption:\r\n- The limit tuples that one reducer can handle: 10000\r\n- Data size: 1311826\r\n\r\nSampling: \r\n- r = limitTuplesPerReducer / dataSize = 10000 / 1311826 ~ 0.0076\r\n- N = 100 / r + 1 = 13119;\r\n- Result: two regions are unfriendly reducers which are (all, all, all, all), (all, month, all, all) and (all, all, all, distance) with the partition factor is 2.7%\r\n\r\nBatching:\r\n*A: month, B: flightnum, C: des, D: distance*\r\n\r\n5 patches:\r\n- (all, all, all, all) & A\r\n- C D CD\r\n- B BC BD BCD\r\n- AC AD ACD\r\n- AB ABD ABC ABCD\r\n\r\n- Number of intermediate keys: 6 559 130 = 1311826 * 5 (batches)\r\n- Size of intermediate keys: 762 MB\r\n\r\nExperiments\r\n-----------------\r\n- Dataset: I used 6 datasets (5 dataset are synthetic, and the other is real data - ISH dataset)\r\n- All the experiments are run under Hadoop cluster of Distributed Systems and Cloud Computing LAB at EURECOM\r\n- Cluster information:  17 slave machines (8GB RAM and a 4-core CPU) with 2 map and 2 reduce slot each\r\n- All results shown in the following are the average of 5 runs\r\n- I used three main metrics for evaluation: runtime – i.e. job execution time – and total amount of work, i.e. the sum of individual task execution times, and phase runtime - i.e. map execution time, reducer execution time...\r\n- Details of result can be viewed in [this document](/document/Experiment Result.pdf)\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}